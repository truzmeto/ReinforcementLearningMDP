---
title: "CS7641 Machine Learning Assignment 4 by Georgia Tech: Markov Decision Process"
author: "T. Ruzmetov"
date: "November 20, 2017"
output:
    pdf_document:
        fig_caption: yes
urlcolor: blue
---

\fontfamily{cmr}
\fontsize{10}{15}
\selectfont


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


*Ownership of the following report developed as a result of assigned institutional effort, an assignment of the CS 7641 Machine Learning course shall reside with GT and the instructors of this class. If the document is released into the public domain it will violate the GT Honor Code*




```{r, echo=FALSE}
library("lattice")
library("plyr")
library("ggplot2")
library("Rmisc")
```


```{r data_preprocessing, echo=FALSE,eval=TRUE}
system("tail -7 output/easy.txt > output/easy_reward.txt
        tail -14 output/easy.txt | head -7 > output/easy_time.txt
        tail -21 output/easy.txt | head -7 > output/easy_steps.txt")

system("tail -7 output/hard.txt > output/hard_reward.txt
        tail -14 output/hard.txt | head -7 > output/hard_time.txt
        tail -21 output/hard.txt | head -7 > output/hard_steps.txt")

# function to reshape input data
reshape_data <- function(data_in) {
        col_names <- c("Iterations","Value","Policy","QLearning") #data_in$V1
        data_in$V1 <- NULL
        data_out <- t(data_in)
        row.names(data_out) <- NULL
        colnames(data_out) <- col_names
        data_out <- as.data.frame(data_out)
}
```


```{r read_data,echo=FALSE,eval=TRUE}
Easy_steps <- read.table("output/easy_steps.txt",sep = "," ,skip = 3)
Easy_reward <- read.table("output/easy_reward.txt",sep = "," ,skip = 3)
Easy_time <- read.table("output/easy_time.txt",sep = "," ,skip = 3)
Easy_steps <- reshape_data(Easy_steps)
Easy_reward <- reshape_data(Easy_reward)
Easy_time <- reshape_data(Easy_time)

Hard_steps <- read.table("output/hard_steps.txt",sep = "," ,skip = 3)
Hard_reward <- read.table("output/hard_reward.txt",sep = "," ,skip = 3)
Hard_time <- read.table("output/hard_time.txt",sep = "," ,skip = 3)
Hard_steps <- reshape_data(Hard_steps)
Hard_reward <- reshape_data(Hard_reward)
Hard_time <- reshape_data(Hard_time)
```



## Introduction 

In this project, we explore Markov Decision Process on two grid world problems. One with small number of states(6x6 grid), while
the other one has large number of states(13x13). Initial and finale positions for an agent is same, but structure of grid(maze) is
different. We solve both problems via three different approaches: value iteration, policy iteration and Q-learning.       

\begin{figure}
    \center
        \includegraphics[width=12.0cm]{plots/Both_Poblems.pdf}
    \center
  \caption{Here we show 6x6(left plot) easy and 13x13(right plot) hard grid world problem topologies. As depicted, grey circle is the agent and
  blue square at top right corner is final destination.}
  \label{fig:both_probs}
\end{figure}

## Problem Setup. Why it is interesting?




## Markov Decision Process

MDP is stochastic decision making proceess, where  the policy learnt by an agent for
selectiong next action is based on current observed state and action. Besides, this new
policy depends only on nearest previous state and action and not on earlier states which is
called Markovian property. The estimations made based on the Markovian property are as good
as the estimations made with the knowledge of the full history up to that point.


## Value Iteration

Value iteration uses Belmans equation, which essentialy states that
the true value (or utility) of a state is the immediate reward for that state, plus
the expected discounted reward if the agent acted optimally from that point on:

$$U(s) = R(s) + \gamma argmax_{a} \sum_{s^{'}}T(s,a,s^{'})U(s^{'})$$

1.  Assign each state a random value
2.  For each state, calculate its new utility based on its neighbor’s utilities.
3.  Update each state’s utility based on the calculation above: $U_{i+1}(s) = R(s) + \gamma argmax_{a} \sum_{s^{'}}T(s,a,s^{'})U_{i}(s^{'})$
4.  Stop upon convergence.

This algorithm is guaranteed to converge to the optimal solutions.


## Policy Iiteration

In policy iteration method we directly and iteratively update policy untill we reach optimal policy. 
We can do so by modifying value iteration to iterate over policies. We start with a random policy, compute
each state’s utility given that policy, and then select a new optimal policy. Here is the algorithm:

1. Randomly initilize policies for all states
2. Evaluate $U_{t}=U^{\pi_{t}}$ given $\pi_{t}$
3. Improve $\pi_{t+1} = argmax_{a} \sum{T(s,a,s^{'})U_{t}(s^{'})}$

and Belman's equation simplifies into n linear equations with n unknowns that is easy to solve:
$U_{i+1}(s) = R(s) + \gamma \sum_{s^{'}}T(s,\pi_{t}(s),s^{'})U_{i}(s^{'})$


## Q-Learning

Value Iteration and Policy Iterations rely on domain knowledge, where agent accurately
knows the transition function and the reward for all states in the environment. In contrast,
in Q-lerning learns a policy or value function directly from experience (model-free).
Q-learning estimates optimal Q-values of an MDP, where its behavior is dictated by
taking actions greedily with respect to the learned Q-values[4].


1. Arbitrarily initialize Q-values for all state-action pairs
2. Then, until learning converges...
    * Choose an action (a) in the current world state (s) based on current Q-value estimates.
    * Take the action (a) and observe the the outcome state ($s^{'}$) and reward (r).
    * Update  $Q(s,a) = Q(s,a) + \alpha [r + \gamma max_{a^{'}} Q(s^{'},a^{'}) Q(s,a)]$      


## Easy Grid World Problem

Easy Grid World problem composed of 6x6 grid, where agent starts its journey at bottom left corner and
by finding optimal policy should end its journey at top right corner for a given structure of maze Fig1.
All three algorithms are applied to this problem. Optimal policies are illustrated in Fig2, where V-values
are shown via heatmap: blue for high V-values and red for low V-values. Value iteration and policy iteration
algorithms show similar optimal policy at 300 iterations. They both converge as early as 100 iterations Fig3.


\begin{figure}
    \center
        \includegraphics[width=16.0cm]{plots/Easy_OptMaps.pdf}
    \center
  \caption{Finale, optimal rout is depicted for all three different algorithms with the heat map.}
  \label{fig:easy_opt_map}
\end{figure}

```{r,echo=FALSE,fig.height=4.,fig.width=12,fig.cap= "Easy problem results"}

p1 <- ggplot(Easy_steps , aes(x=Iterations)) +
    geom_line(aes(y = Value, colour = "Value")) + 
    geom_line(aes(y = Policy, colour = "Policy")) +
    geom_line(aes(y = QLearning, colour = "QLearning")) +
    #geom_point() +
    theme_bw() +
    labs(title = "", y = "Steps", color="") +
    theme(legend.position = "none", axis.title = element_text(size = 12.0),
        axis.text = element_text(size=8, face = "bold"),
        plot.title = element_text(size = 12, hjust = 0.5),
        #text = element_text(family="Times New Roman"),
        axis.text.x = element_text(colour="black"),
        axis.text.y = element_text(colour="black"))

p2 <- ggplot(Easy_reward , aes(x=Iterations)) +
    geom_line(aes(y = Value, colour = "Value")) + 
    geom_line(aes(y = Policy, colour = "Policy")) +
    geom_line(aes(y = QLearning, colour = "QLearning")) +
    #geom_point() +
    theme_bw() +
    labs(title = "", y = "Reward", color="") +
    theme(legend.position = "none",
          axis.title = element_text(size = 12.0),
          axis.text = element_text(size=8, face = "bold"),
          plot.title = element_text(size = 12, hjust = 0.5),
          #text = element_text(family="Times New Roman"),
          axis.text.x = element_text(colour="black"),
          axis.text.y = element_text(colour="black"))

p3 <- ggplot(Easy_time, aes(x=Iterations)) +
    geom_line(aes(y = Value , colour = "Value")) + 
    geom_line(aes(y = Policy, colour = "Policy")) +
    geom_line(aes(y = QLearning, colour = "QLearning")) +
    #geom_point() +
    theme_bw() +
    labs(title = "", y = "Time(ms)", color="") +
    theme(legend.position = c(0.2,0.8),
        axis.title = element_text(size = 12.0),
        axis.text = element_text(size=8, face = "bold"),
        plot.title = element_text(size = 12, hjust = 0.5),
        #text = element_text(family="Times New Roman"),
        axis.text.x = element_text(colour="black"),
        axis.text.y = element_text(colour="black"))


multiplot(p1,p2,p3,cols=3)
```




## Hard Grid World Problem


\begin{figure}
    \center
        \includegraphics[width=16.0cm]{plots/Hard_OptMaps.pdf}
    \center
  \caption{}
  \label{fig:hard_opt_map}
\end{figure}








```{r,echo=FALSE,fig.height=4.,fig.width=12,fig.cap= "Hard problem results"}

p1 <- ggplot(Hard_steps , aes(x=Iterations)) +
    geom_line(aes(y = Value, colour = "Value")) + 
    geom_line(aes(y = Policy, colour = "Policy")) +
    geom_line(aes(y = QLearning, colour = "QLearning")) +
    #geom_point() +
    theme_bw() +
    labs(title = "", y = "Steps", color="") +
    theme(legend.position =c(0.7,0.8), axis.title = element_text(size = 12.0),
        axis.text = element_text(size=8, face = "bold"),
        plot.title = element_text(size = 12, hjust = 0.5),
        #text = element_text(family="Times New Roman"),
        axis.text.x = element_text(colour="black"),
        axis.text.y = element_text(colour="black"))

p2 <- ggplot(Hard_reward , aes(x=Iterations)) +
    geom_line(aes(y = Value, colour = "Value")) + 
    geom_line(aes(y = Policy, colour = "Policy")) +
    geom_line(aes(y = QLearning, colour = "QLearning")) +
    #geom_point() +
    theme_bw() +
    labs(title = "", y = "Reward", color="") +
    theme(legend.position = "none",
          axis.title = element_text(size = 12.0),
          axis.text = element_text(size=8, face = "bold"),
          plot.title = element_text(size = 12, hjust = 0.5),
          #text = element_text(family="Times New Roman"),
          axis.text.x = element_text(colour="black"),
          axis.text.y = element_text(colour="black"))

p3 <- ggplot(Hard_time, aes(x=Iterations)) +
    geom_line(aes(y = Value , colour = "Value")) + 
    geom_line(aes(y = abs(Policy), colour = "Policy")) +
    geom_line(aes(y = QLearning, colour = "QLearning")) +
    #geom_point() +
    theme_bw() +
    labs(title = "", y = "Time(ms)", color="") +
    theme(legend.position = "none", axis.title = element_text(size = 12.0),
        axis.text = element_text(size=8, face = "bold"),
        plot.title = element_text(size = 12, hjust = 0.5),
        #text = element_text(family="Times New Roman"),
        axis.text.x = element_text(colour="black"),
        axis.text.y = element_text(colour="black"))


multiplot(p1,p2,p3,cols=3)
```


\pagebreak


### Q-Learning

\pagebreak


## Conclusion



## References

1. T. M. Mitchell, Machine Learning. McGraw Hill, 1997.
2. L. P. Kaelbling, M. Littman, and A. Moore, ’Reinforcement learning: A survey,’ Journal of Artificial Intelligence Research, vol. 4, pp. 237-285, 1996.7
3. R. S. Sutton and A. G. Barto, Reinforcement Learning: An Introduction. London, England: The MIT Press, 1998.
4. http://burlap.cs.brown.edu/index.html
5. https://github.com/truzmeto/RL_MDP
6. https://github.com/juanjose49/omscs-cs7641-machine-learning-assignment-4


