
The data below shows the number of steps/actions the agent required to reach 
the terminal state given the number of iterations the algorithm was run.
Iterations,10,20,30,40,50,60,70,80,90,100,110,120,130,140,150,160,170,180,190,200,210,220,230,240,250,260,270,280,290,300,310,320,330,340,350,360,370,380,390,400,410,420,430,440,450,460,470,480,490,500,510,520,530,540,550,560,570,580,590,600,610,620,630,640,650,660,670,680,690,700,710,720,730,740,750,760,770,780,790,800,810,820,830,840,850,860,870,880,890,900,910,920,930,940,950,960,970,980,990,1000
Value Iteration,124,35,31,28,28,26,39,49,32,49,39,44,38,41,26,27,47,31,34,28,32,32,34,37,37,34,37,34,28,33,28,49,44,32,34,34,33,36,41,29,28,37,37,30,38,41,36,32,29,36,31,27,44,38,37,33,31,40,35,31,30,45,36,30,39,36,30,35,34,31,32,39,37,35,28,26,32,43,36,38,35,26,40,39,41,44,39,32,31,42,39,38,33,40,34,42,35,34,35,34
Policy Iteration,878,30,26,34,31,36,32,29,32,44,37,35,33,36,35,36,39,34,36,45,35,28,36,29,31,29,34,30,35,35,38,29,35,34,34,32,31,37,28,27,43,31,42,36,32,31,31,37,47,32,39,31,38,40,35,38,32,40,30,33,46,30,32,40,31,34,34,33,35,36,31,48,32,34,36,41,32,36,28,33,34,30,31,33,29,34,31,34,36,39,38,31,28,37,35,28,33,48,31,35
Q Learning,175,94,253,119,38,46,40,34,30,36,63,31,33,32,43,60,42,54,52,35,40,41,44,120,55,32,47,55,38,51,46,38,45,45,40,56,35,39,67,72,46,48,54,36,66,30,43,53,56,81,49,36,76,54,64,31,36,58,36,45,31,38,29,41,42,33,44,87,54,27,41,52,36,103,36,44,58,61,38,40,40,60,56,41,36,48,54,33,40,32,52,38,80,47,40,37,61,40,40,30
