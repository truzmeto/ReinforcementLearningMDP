
The data below shows the number of steps/actions the agent required to reach 
the terminal state given the number of iterations the algorithm was run.
Iterations,10,20,30,40,50,60,70,80,90,100,110,120,130,140,150,160,170,180,190,200,210,220,230,240,250,260,270,280,290,300,310,320,330,340,350,360,370,380,390,400,410,420,430,440,450,460,470,480,490,500,510,520,530,540,550,560,570,580,590,600,610,620,630,640,650,660,670,680,690,700,710,720,730,740,750,760,770,780,790,800,810,820,830,840,850,860,870,880,890,900,910,920,930,940,950,960,970,980,990,1000
Value Iteration,86,40,39,38,31,26,40,27,29,47,27,37,31,37,33,29,37,38,29,37,32,35,30,40,37,31,37,29,28,35,32,38,38,30,40,34,36,39,36,39,43,31,28,38,31,47,32,49,34,43,38,33,28,28,35,33,33,36,41,41,32,34,38,34,33,37,30,42,35,37,33,42,33,33,42,36,44,46,42,31,35,35,39,41,33,37,31,35,35,28,35,30,34,40,31,37,31,33,33,29
Policy Iteration,1297,39,34,38,36,32,27,30,30,45,41,25,32,29,31,40,37,32,37,29,36,56,35,32,43,31,29,39,31,32,30,31,30,27,37,38,37,40,29,32,26,32,29,36,34,31,31,34,29,41,33,31,32,31,40,38,30,35,50,29,29,33,38,37,43,44,35,37,39,31,41,30,30,27,34,33,30,29,33,31,30,34,29,40,35,41,30,29,35,32,28,28,32,35,43,33,38,27,34,44
Q Learning,518,171,54,79,225,47,172,101,41,58,68,48,73,82,87,77,119,104,76,49,54,68,134,51,71,43,100,47,95,76,44,163,269,40,203,43,57,118,85,54,40,79,45,104,77,95,49,53,90,165,52,51,65,47,78,35,80,95,50,41,78,67,77,111,108,63,115,69,149,64,126,96,67,64,110,52,53,135,90,55,66,41,94,38,74,62,67,37,100,77,60,37,52,120,65,59,50,39,57,151
