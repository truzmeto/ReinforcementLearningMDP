
The data below shows the number of milliseconds the algorithm required to generate 
the optimal policy given the number of iterations the algorithm was run.
Iterations,10,20,30,40,50,60,70,80,90,100,110,120,130,140,150,160,170,180,190,200,210,220,230,240,250,260,270,280,290,300,310,320,330,340,350,360,370,380,390,400,410,420,430,440,450,460,470,480,490,500,510,520,530,540,550,560,570,580,590,600,610,620,630,640,650,660,670,680,690,700,710,720,730,740,750,760,770,780,790,800,810,820,830,840,850,860,870,880,890,900,910,920,930,940,950,960,970,980,990,1000
Value Iteration,117,115,166,170,115,102,105,130,142,149,170,187,226,201,225,227,250,289,275,295,311,327,319,331,338,357,397,365,398,403,425,429,452,456,484,481,507,502,543,550,556,580,562,604,617,603,641,665,642,666,696,716,669,714,746,760,786,798,760,791,810,826,852,865,879,896,912,925,939,951,956,964,963,983,981,984,992,1009,1075,1077,1075,1079,1086,1142,1144,1141,1131,1193,1182,1171,1235,1223,1215,1260,1238,1232,1287,1268,1321,1300
Policy Iteration,54,77,92,98,118,140,170,188,208,236,253,274,301,321,346,369,389,415,438,456,477,509,530,556,624,615,618,686,723,689,717,726,747,762,789,928,898,876,895,924,924,945,973,992,1023,1048,1081,1105,1167,1133,1192,1178,1188,1372,1287,1299,1291,1301,1330,1363,1381,1398,1420,1430,1473,1483,1510,1586,1565,1653,1603,1641,1646,1845,1761,1733,1737,1768,1781,1809,1884,1937,2033,1970,1985,2029,1974,-2106,2109,-2000,2088,2048,2071,2096,2136,-2043,-2089,-2065,-2069,-2047
Q Learning,79,50,65,49,48,44,52,50,59,58,71,62,62,62,73,72,82,81,80,91,99,104,116,94,104,95,109,115,115,111,118,120,117,107,125,130,133,124,129,145,127,144,148,145,146,157,178,167,178,158,170,174,172,163,180,194,184,173,204,213,187,217,204,200,209,197,219,209,251,240,205,223,224,213,243,241,287,238,232,236,284,283,327,293,246,260,256,261,299,297,291,274,272,271,291,300,299,295,287,305
