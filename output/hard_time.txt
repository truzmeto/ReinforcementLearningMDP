
The data below shows the number of milliseconds the algorithm required to generate 
the optimal policy given the number of iterations the algorithm was run.
Iterations,10,20,30,40,50,60,70,80,90,100,110,120,130,140,150,160,170,180,190,200,210,220,230,240,250,260,270,280,290,300,310,320,330,340,350,360,370,380,390,400,410,420,430,440,450,460,470,480,490,500,510,520,530,540,550,560,570,580,590,600,610,620,630,640,650,660,670,680,690,700,710,720,730,740,750,760,770,780,790,800,810,820,830,840,850,860,870,880,890,900,910,920,930,940,950,960,970,980,990,1000
Value Iteration,66,87,75,74,51,66,71,120,106,97,107,120,162,135,144,151,160,209,179,189,194,205,212,222,231,241,249,313,270,281,290,338,314,388,384,334,386,351,403,380,394,387,395,431,472,425,493,441,454,493,480,485,535,521,512,520,554,533,558,560,620,573,592,650,654,609,619,636,639,650,725,793,851,758,712,813,790,763,792,772,908,897,806,835,887,814,938,867,878,873,892,919,855,911,1002,990,891,924,1039,984
Policy Iteration,30,54,82,96,99,119,136,155,201,244,228,261,247,268,343,368,396,338,389,376,398,473,478,461,523,499,548,528,547,623,698,603,689,640,672,677,776,851,791,815,770,891,804,872,1014,911,979,961,1032,939,985,1036,1053,1098,1077,1151,1159,1128,1181,1198,1145,1316,1188,1223,1295,1355,1333,1324,1339,1333,1465,1515,1655,1413,1472,1505,1499,1581,1572,1616,1619,1748,1680,1654,1727,1760,1639,1691,1739,1815,1808,1786,1793,1841,1902,1805,1997,1989,2043,1977
Q Learning,50,47,41,48,37,46,42,45,37,46,39,42,48,50,41,44,49,47,56,58,73,60,72,80,59,83,73,65,63,67,74,73,82,72,83,83,76,83,80,81,87,87,92,84,83,100,96,92,120,94,103,95,102,104,111,121,136,108,108,111,112,109,115,117,162,117,140,161,128,117,121,124,130,183,187,194,193,155,173,138,140,137,139,139,142,158,141,195,159,185,213,185,233,166,199,212,156,177,182,170
