
The data below shows the number of steps/actions the agent required to reach 
the terminal state given the number of iterations the algorithm was run.
Iterations,3,6,9,12,15,18,21,24,27,30,33,36,39,42,45,48,51,54,57,60,63,66,69,72,75,78,81,84,87,90,93,96,99,102,105,108,111,114,117,120,123,126,129,132,135,138,141,144,147,150,153,156,159,162,165,168,171,174,177,180,183,186,189,192,195,198,201,204,207,210,213,216,219,222,225,228,231,234,237,240,243,246,249,252,255,258,261,264,267,270,273,276,279,282,285,288,291,294,297,300
Value Iteration,67,19,16,18,12,12,15,13,11,16,13,13,13,12,16,16,12,15,12,16,14,15,12,11,14,20,13,14,13,11,15,15,11,15,14,14,13,17,14,12,22,11,16,16,13,11,13,13,12,12,19,17,15,12,12,13,12,19,23,12,15,13,11,14,14,18,12,13,13,18,15,12,11,12,14,15,13,13,17,14,11,11,11,14,23,13,19,11,11,11,15,13,12,12,14,15,14,17,17,15
Policy Iteration,727,18,11,14,11,14,21,16,14,12,13,12,14,15,17,14,12,14,13,13,15,12,12,11,16,14,11,12,15,14,12,13,13,15,13,14,16,15,12,15,19,16,14,15,13,13,11,11,15,13,21,12,11,22,18,11,14,13,16,11,16,12,15,11,14,18,12,14,13,12,13,13,15,14,11,11,15,11,14,18,13,14,14,15,12,14,12,13,13,12,16,20,11,18,14,12,17,19,16,29
Q Learning,83,69,20,27,48,17,24,16,11,15,18,117,197,20,16,17,44,17,23,16,18,19,12,18,16,19,11,44,24,18,62,18,15,18,40,12,59,21,18,26,16,25,16,20,17,48,25,22,26,27,12,23,26,20,14,27,17,19,21,12,44,24,15,94,14,22,21,15,24,16,27,15,15,19,29,76,16,21,26,14,23,21,17,16,14,16,13,12,17,17,26,32,22,22,27,14,14,20,19,12
