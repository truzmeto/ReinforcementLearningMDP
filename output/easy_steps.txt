
The data below shows the number of steps/actions the agent required to reach 
the terminal state given the number of iterations the algorithm was run.
Iterations,3,6,9,12,15,18,21,24,27,30,33,36,39,42,45,48,51,54,57,60,63,66,69,72,75,78,81,84,87,90,93,96,99,102,105,108,111,114,117,120,123,126,129,132,135,138,141,144,147,150,153,156,159,162,165,168,171,174,177,180,183,186,189,192,195,198,201,204,207,210,213,216,219,222,225,228,231,234,237,240,243,246,249,252,255,258,261,264,267,270,273,276,279,282,285,288,291,294,297,300
Value Iteration,23,14,14,20,14,14,12,14,13,18,20,13,14,11,12,14,11,12,11,29,11,12,15,11,18,17,21,18,15,19,13,12,14,18,16,12,11,11,12,14,12,13,11,11,15,14,12,16,11,12,13,16,12,14,11,15,13,17,15,12,25,14,11,14,14,14,14,17,12,12,12,19,15,11,21,11,18,17,13,11,15,17,16,13,13,13,14,17,13,11,14,14,12,11,13,15,11,14,17,12
Policy Iteration,204,24,14,20,11,15,13,13,13,19,15,20,15,14,14,14,11,11,20,15,11,11,12,13,11,16,17,19,13,25,12,20,14,14,12,12,14,14,12,15,13,11,16,14,14,13,18,14,18,14,12,12,13,14,13,16,15,26,16,13,14,16,12,14,13,17,15,16,12,14,13,13,15,13,13,12,17,12,15,12,13,18,14,15,14,16,16,15,13,17,14,13,20,16,11,16,14,15,13,16
Q Learning,124,43,49,23,29,38,17,20,18,19,27,14,17,19,14,21,14,12,16,28,13,25,13,12,15,12,22,19,13,12,13,15,16,11,14,17,15,33,14,19,16,14,21,20,13,12,14,18,12,13,16,20,14,11,19,15,21,20,26,14,16,14,12,23,68,14,17,18,23,15,13,33,18,23,14,16,20,25,11,21,27,14,13,22,18,12,20,20,16,18,17,12,14,19,15,14,79,14,17,20
