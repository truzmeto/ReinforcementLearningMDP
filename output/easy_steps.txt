
The data below shows the number of steps/actions the agent required to reach 
the terminal state given the number of iterations the algorithm was run.
Iterations,3,6,9,12,15,18,21,24,27,30,33,36,39,42,45,48,51,54,57,60,63,66,69,72,75,78,81,84,87,90,93,96,99,102,105,108,111,114,117,120,123,126,129,132,135,138,141,144,147,150,153,156,159,162,165,168,171,174,177,180,183,186,189,192,195,198,201,204,207,210,213,216,219,222,225,228,231,234,237,240,243,246,249,252,255,258,261,264,267,270,273,276,279,282,285,288,291,294,297,300
Value Iteration,20,12,22,12,12,13,21,12,12,13,12,13,20,12,11,17,11,13,17,15,25,13,13,13,14,17,16,12,18,12,13,14,17,12,14,20,13,13,11,15,17,16,12,13,11,17,11,12,19,18,15,13,18,12,12,13,14,15,14,22,15,12,16,17,18,13,14,11,13,17,14,15,11,14,12,12,19,17,15,11,13,13,15,15,17,13,13,14,16,12,13,14,14,12,15,14,11,18,15,17
Policy Iteration,264,18,13,11,11,12,12,20,13,13,16,15,14,17,16,13,12,14,11,14,12,12,13,16,14,16,12,12,13,14,14,14,14,12,11,11,11,16,12,15,13,14,18,11,14,17,14,17,13,11,15,11,15,11,11,12,12,15,11,17,13,14,15,11,15,15,11,15,23,16,17,16,15,17,13,13,11,13,13,14,15,12,12,16,12,16,14,11,12,11,22,14,14,13,11,18,13,11,18,11
Q Learning,105,36,20,20,13,20,15,12,26,26,13,19,16,13,37,21,37,25,14,31,148,26,27,12,19,22,14,15,50,16,17,78,17,17,17,15,18,17,20,20,13,22,16,28,13,12,15,13,12,12,15,24,16,17,17,17,41,13,22,18,18,24,14,12,24,22,18,12,16,20,14,17,42,11,14,19,17,18,20,18,14,15,15,11,19,28,18,14,17,15,17,15,27,14,19,14,13,25,16,12
